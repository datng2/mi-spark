{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform MachineInfo from RDD to DataFrames\n",
    "\n",
    "* Import `PsPipeline`\n",
    "* Construct `miRDDReader`\n",
    "* Transform `miRDDs` to `miDF`\n",
    "* Perform/Benchmark some queries on `miDF`\n",
    "\n",
    "#### Overview\n",
    "This notebook aims to experiment whether:\n",
    " 1. Spark DataFrames \\would feasibly allow us to perform complex SQL queries on MachineInfo at scale.\n",
    " 2. Saving file as `Parquet` (column-based) could reduce storage cost.\n",
    "\n",
    "#### TODO\n",
    "\n",
    "* Fix `miReader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inject `PsPipline `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///Users/datng2/pspipline.jar\n",
      "Finished download of pspipline.jar\n"
     ]
    }
   ],
   "source": [
    "// Import extra dependency using Jar\n",
    "// Ref: https://toree.apache.org/docs/current/user/faq/\n",
    "%AddJar file:///Users/datng2/pspipline.jar\n",
    "sc.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@fe55ca1\n",
       "sqlContext = org.apache.spark.sql.SQLContext@45773d74\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@45773d74"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "// https://stackoverflow.com/questions/44094108/not-able-to-import-spark-implicits-in-scalatest\n",
    "val spark2 = spark\n",
    "val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n",
    "import spark2.implicits._\n",
    "import sqlContext.implicits._\n",
    "\n",
    "import java.net.URI\n",
    "import collection.JavaConverters._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import com.tetration.proto.MachineInfoProtos.MachineInfo\n",
    "import com.tetration.processanalytics.pipeline.io.MachineInfoBatchReaderWithProtoField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Dataframes from RDD\n",
    "\n",
    "```\n",
    "/* \n",
    "*      Vanilla schemas to represent relationship between\n",
    "*  MachineInfo (1) <-------> (N) ProcessInfo (1) <-------> (N) ForensicEvent\n",
    "*\n",
    "*/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "miProtoReader = com.tetration.processanalytics.pipeline.io.MachineInfoBatchReaderWithProtoField@72dbe8db\n",
       "miBatches = [201805091902, 201805091903, 201805091904]\n",
       "miRDDs = UnionRDD[13] at union at <console>:67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[13] at union at <console>:67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val miProtoReader = new MachineInfoBatchReaderWithProtoField(\n",
    "    sc,\n",
    "    sc.hadoopConfiguration,\n",
    "    FileSystem.get(URI.create(\"file:///\"), sc.hadoopConfiguration),\n",
    "    \"/Users/datng2/data/machineinfo/galois\")\n",
    "\n",
    "val miBatches = Seq(\"201805091902\", \n",
    "                    \"201805091903\", \n",
    "                    \"201805091904\").asJava\n",
    "\n",
    "val miRDDs = miProtoReader\n",
    "    .readBatches(miBatches).asScala\n",
    "    .reduce(_ union _ )\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define experimental schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ProcessInfoRow\n",
       "defined class SensorIdRow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class ProcessInfoRow(\n",
    "    val sensorId: String,\n",
    "    val processKeyPart1: Long,\n",
    "    val processKeyPart2: Long,\n",
    "    val commandString: String)\n",
    "\n",
    "case class SensorIdRow(val sensorId: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIToProcessInfo = > List[ProcessInfoRow] = <function1>\n",
       "MIToSensorId = > List[SensorIdRow] = <function1>\n",
       "processInfoDF = [sensorId: string, processKeyPart1: bigint ... 2 more fields]\n",
       "sensorIdDF = [sensorId: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.catalyst.QualifiedTableName'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'QualifiedTableName.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sensorId: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform functions\n",
    "val MIToProcessInfo: MachineInfo => List[ProcessInfoRow] = mi => {\n",
    "  mi.getProcessInfoList.asScala\n",
    "    .map(pi => ProcessInfoRow(\n",
    "        mi.getSensorId,\n",
    "        pi.getKey.getPart1,\n",
    "        pi.getKey.getPart2,\n",
    "        pi.getCommandString))\n",
    "    .toList\n",
    "}\n",
    "\n",
    "val MIToSensorId: MachineInfo => List[SensorIdRow] = mi => {\n",
    "    mi.getProcessInfoList.asScala\n",
    "        .map(pi => SensorIdRow(mi.getSensorId))\n",
    "        .toList\n",
    "}\n",
    "\n",
    "val processInfoDF = miRDDs.flatMap(MIToProcessInfo).toDF\n",
    "val sensorIdDF = miRDDs.flatMap(MIToSensorId).toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   11486|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sensorIdDF.createOrReplaceTempView(\"SensorId\")\n",
    "sqlContext.sql(\"SELECT COUNT(*) from SensorId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------+\n",
      "|            sensorId|count(DISTINCT commandString)|\n",
      "+--------------------+-----------------------------+\n",
      "|1f5daca2e393a1ab5...|                          128|\n",
      "|5601456f1b9e75fc6...|                          212|\n",
      "|4a345e6a678d3bb21...|                          127|\n",
      "|fb6bd6990776bce3d...|                          128|\n",
      "|fda22d272815f1503...|                           24|\n",
      "|aff1466a070b40053...|                          239|\n",
      "|c513d16dc8aa88554...|                          130|\n",
      "|7ff2be60e4d97634a...|                          130|\n",
      "|749e050e5494f15d5...|                          419|\n",
      "|d09989fcc31fa0c41...|                          231|\n",
      "+--------------------+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res = [sensorId: string, count(DISTINCT commandString): bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sensorId: string, count(DISTINCT commandString): bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processInfoDF.createOrReplaceTempView(\"ProcessInfo\")\n",
    "val res = sqlContext.sql(\"\"\"\n",
    "    SELECT sensorId, COUNT(DISTINCT commandString) FROM ProcessInfo\n",
    "        GROUP BY sensorId\n",
    "\"\"\")\n",
    "res.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.io.compress.GzipCodec\n",
    "sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "\n",
    "sensorIdDF.write.parquet(\"sensorid.parquet\")\n",
    "processInfoDF.write.parquet(\"pi.parquet\")\n",
    "\n",
    "sensorIdDF.rdd.saveAsTextFile(\"sensorid.rdd\", classOf[GzipCodec])\n",
    "processInfoDF.rdd.saveAsTextFile(\"pi.rdd\", classOf[GzipCodec])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
