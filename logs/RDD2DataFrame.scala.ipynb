{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically Specifying the Schema\n",
    "\n",
    "Ref: https://spark.apache.org/docs/2.0.2/sql-programming-guide.html#programmatically-specifying-the-schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create RDD of `Rows` from original RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.net.BindException\n",
       "Message: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
       "StackTrace:   at sun.nio.ch.Net.bind0(Native Method)\n",
       "  at sun.nio.ch.Net.bind(Net.java:433)\n",
       "  at sun.nio.ch.Net.bind(Net.java:425)\n",
       "  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n",
       "  at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)\n",
       "  at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\n",
       "  at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)\n",
       "  at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\n",
       "  at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\n",
       "  at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)\n",
       "  at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\n",
       "  at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)\n",
       "  at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n",
       "  at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n",
       "  at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n",
       "  at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
       "  at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Simulate RDD of rows\n",
    "val rowRDD = sc.parallelize(Array(\n",
    "    Row(\"dat_pc1\", \"10.2.3.5\",\"asda4:1241:4124\"),\n",
    "    Row(\"dat_mac1\", \"10.2.3.4\",\"bas4:1241:4125\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the schema represented by a `StructType` matching the structure of `Rows` in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(host_name,StringType,true), StructField(ip_address,StringType,true), StructField(mac_address,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(host_name,StringType,true), StructField(ip_address,StringType,true), StructField(mac_address,StringType,true))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(\n",
    "    StructField(\"host_name\", StringType, true),\n",
    "    StructField(\"ip_address\",StringType, true),\n",
    "    StructField(\"mac_address\",StringType, true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply the schema to RDD of Rows via `createDataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [host_name: string, ip_address: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[host_name: string, ip_address: string ... 1 more field]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Construct DataFrame from RDD\n",
    "val df = spark.createDataFrame(rowRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creates a temporary view `HostInfo` using the DataFrame\n",
    "df.createOrReplaceTempView(\"HostInfo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query DataFrame using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+\n",
      "|host_name|ip_address|    mac_address|\n",
      "+---------+----------+---------------+\n",
      "|  dat_pc1|  10.2.3.5|asda4:1241:4124|\n",
      "| dat_mac1|  10.2.3.4| bas4:1241:4125|\n",
      "+---------+----------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results = [host_name: string, ip_address: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[host_name: string, ip_address: string ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = spark.sql(\"SELECT * FROM HostInfo LIMIT 10\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- mac_address: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|host_name|\n",
      "+---------+\n",
      "|  dat_pc1|\n",
      "| dat_mac1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"host_name\").limit(10).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
