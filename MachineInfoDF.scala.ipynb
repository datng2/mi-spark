{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform MachineInfo from RDD to DataFrames\n",
    "\n",
    "### TL;DR:\n",
    "\n",
    "`Parqet` seems to reduce storage cost and `DataFrames` allows to make complex queries.\n",
    "\n",
    "### Motivation\n",
    "Expose information from MachineInfo as Spark’s DataFrames to provide SQL-based offline or on-demand analysis capabilities.\n",
    "\n",
    "### Overview\n",
    "* Extract and flatten MachineInfo to a couple of tables such as Processes, Events, Workloads, etc. Save the table in column-oriented `parquet` format to reduce the required disk space and hence improve the retention of these tables.\n",
    "* Explore other ways to reduce the disk usage.\n",
    "* Propose a couple of real-life usages of these DataFrames for Threat Hunting using Spark’s built-in aggregation functions and plugged-in user-defined functions.\n",
    "\n",
    "### Future work\n",
    "* Convert experimental code into `PsPipline` codebase\n",
    "* Write tests\n",
    "\n",
    "\n",
    "### Open question: \n",
    "    * What if we would like to extract a part of RDD. Can we have a \"loose\" schema to extract only needed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import  dependencies\n",
    "\n",
    "* Assume that `PsPipeline` has been compiled into a jar file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///Users/datng2/pspipline.jar\n",
      "Finished download of pspipline.jar\n"
     ]
    }
   ],
   "source": [
    "// Ref: https://toree.apache.org/docs/current/user/faq/\n",
    "%AddJar file:///Users/datng2/pspipline.jar\n",
    "sc.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@6e60574d\n",
       "sqlContext = org.apache.spark.sql.SQLContext@107986c4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@107986c4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "// https://stackoverflow.com/questions/44094108/not-able-to-import-spark-implicits-in-scalatest\n",
    "val spark2 = spark\n",
    "val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n",
    "import spark2.implicits._\n",
    "import sqlContext.implicits._\n",
    "\n",
    "import java.net.URI\n",
    "import collection.JavaConverters._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import com.tetration.proto.MachineInfoProtos.MachineInfo\n",
    "import com.tetration.processanalytics.pipeline.io.MachineInfoBatchReaderWithProtoField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load `RDD[MachineInfo]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATA_DIR = /Users/datng2/data/machineinfo/galois\n",
       "NUM_BATCHES = 5\n",
       "batches = WrappedArray(201805091922, 201805092101, 201805091925, 201805091913, 201805091914)\n",
       "miProtoReader = com.tetration.processanalytics.pipeline.io.MachineInfoBatchReaderWithProtoField@6f439199\n",
       "miRDDs = UnionRDD[23] at union at <console>:77\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[23] at union at <console>:77"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.URI\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "final val DATA_DIR = \"/Users/datng2/data/machineinfo/galois\"\n",
    "final val NUM_BATCHES = 5\n",
    "\n",
    "// Print all files in a directory\n",
    "val batches = FileSystem\n",
    "    .get(URI.create(\"file:///\"), sc.hadoopConfiguration)\n",
    "    .listStatus(new Path(DATA_DIR))\n",
    "    .map(_.getPath.toString.split(\"/\").last)  // extract filename from an absolute path\n",
    "    .take(NUM_BATCHES)  // select first N-items \n",
    "    .toSeq    //convert to seq\n",
    "\n",
    "val miProtoReader = new MachineInfoBatchReaderWithProtoField(\n",
    "    sc,\n",
    "    sc.hadoopConfiguration,\n",
    "    FileSystem.get(URI.create(\"file:///\"), sc.hadoopConfiguration),\n",
    "    DATA_DIR)\n",
    "\n",
    "val miRDDs = miProtoReader\n",
    "    .readBatches(batches.asJava).asScala\n",
    "    .reduce(_ union _)\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "host_name: \"collectorDatamover-2\"\n",
       "sensor_id: \"fda22d272815f15031cb91af50df1fa7621c310c\"\n",
       "process_info {\n",
       "  process_id: 2396\n",
       "  user_id: \"collectd\"\n",
       "  parent_process_id: 24429\n",
       "  command_string: \"ping\\000-c2\\00010.28.1.193\\000\"\n",
       "  start_time_usec: 1525893688000000\n",
       "  end_time_usec: 1525893689637822\n",
       "  exec_path: \"/bin/ping\"\n",
       "  file_attr {\n",
       "    size: 40760\n",
       "    ctime: 1486575379552999998\n",
       "    mtime: 1380206111000000000\n",
       "    mode: 35309\n",
       "  }\n",
       "  thread_group_id: 2396\n",
       "  parent_thread_group_id: 24429\n",
       "  exec_time_usec: 1525893688634884\n",
       "  privilege_escalated: true\n",
       "  exit_code: 0\n",
       "  exit_signal: 17\n",
       "  username: \"collectd\"\n",
       "  uid: 513\n",
       "  uptime_usec: 1000000\n",
       "  forensic_events {\n",
       "    security_event: PRIV_ESCALATION\n",
       "    priv_escalation_data {\n",
       "      types_bitmap...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val test = miRDDs.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106716"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.getProcessInfoList.get(0).getMemoryUsageKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define  an experimental schema\n",
    "\n",
    "* The idea is to convert **RDD[MachineInfo] into RDD[Row]**, where each Row contains the information that our pipeline requires. In Spark 2.0, DataFrame is a type alias for *Dataset of Row*.\n",
    "* We could potentially consider leveraging `protobuf` definition to transform RDD to DataFrame. However, we currently use a simplified schema for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ProcessInfoRow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class ProcessInfoRow(\n",
    "    val sensorId: String,\n",
    "    val processKeyPart1: Long,\n",
    "    val processKeyPart2: Long,\n",
    "    val cpuUsage: Long,\n",
    "    val memoryUsage: Long,\n",
    "    val commandString: String\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIToProcessInfo = > List[ProcessInfoRow] = <function1>\n",
       "processInfoDF = [sensorId: string, processKeyPart1: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sensorId: string, processKeyPart1: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform functions\n",
    "val MIToProcessInfo: MachineInfo => List[ProcessInfoRow] = mi => {\n",
    "  mi.getProcessInfoList.asScala\n",
    "    .map(pi => ProcessInfoRow(\n",
    "        mi.getSensorId,\n",
    "        pi.getKey.getPart1,\n",
    "        pi.getKey.getPart2,\n",
    "        pi.getCpuUsageUs,\n",
    "        pi.getMemoryUsageKB,\n",
    "        pi.getCommandString))\n",
    "    .toList\n",
    "}\n",
    "\n",
    "val processInfoDF = miRDDs\n",
    "    .flatMap(MIToProcessInfo)\n",
    "    .toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sample Queries:\n",
    "Ref: https://github.com/apache/incubator-toree/blob/master/etc/examples/notebooks/magic-tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3488|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Create a ProcessInfo Table\n",
    "processInfoDF.createOrReplaceTempView(\"ProcessInfo\")\n",
    "\n",
    "// Count number of rows sensorId\n",
    "sqlContext\n",
    "    .sql(\"\"\"SELECT COUNT(*) FROM ProcessInfo\"\"\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------+\n",
      "|            sensorId|count(DISTINCT commandString)|\n",
      "+--------------------+-----------------------------+\n",
      "|fda22d272815f1503...|                          303|\n",
      "|7f3fc376d08f771af...|                          431|\n",
      "|11f991d882bdab1f7...|                          307|\n",
      "|20bee197e8368b81b...|                          315|\n",
      "|a26526ab2b11a0a1c...|                          318|\n",
      "|967d3582a6a9bc0da...|                          398|\n",
      "+--------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Create a SQL query to count number of unique command strings in each sensorId\n",
    "sqlContext\n",
    "    .sql(\"\"\"\n",
    "        SELECT sensorId, COUNT(DISTINCT commandString) FROM ProcessInfo\n",
    "            GROUP BY sensorId\n",
    "    \"\"\")\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       avg(cpuUsage)|\n",
      "+--------------------+\n",
      "|3.1103188016055045E9|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Compute average CPU usage (in microsecond)\n",
    "sqlContext\n",
    "    .sql(\"\"\"\n",
    "        SELECT AVG(cpuUsage) FROM ProcessInfo\n",
    "    \"\"\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Saving to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "processInfoDF.write.parquet(\"pi.parquet\")\n",
    "\n",
    "import org.apache.hadoop.io.compress.GzipCodec\n",
    "processInfoDF.rdd.saveAsTextFile(\"pi.rdd\", classOf[GzipCodec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdd     size: 3964928 kB\n",
      "Parquet size: 786432 kB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fs = org.apache.hadoop.fs.LocalFileSystem@447a4fea\n",
       "path = /Users/datng2/internship/mi-spark/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/Users/datng2/internship/mi-spark/"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fs = FileSystem.get(URI.create(\"file:///\"), sc.hadoopConfiguration)\n",
    "\n",
    "val path = \"/Users/datng2/internship/mi-spark/\"\n",
    "\n",
    "println(\"Rdd     size: \" + fs.listStatus(new Path(path + \"pi.rdd\")).map(x => x.getBlockSize()).sum / 1024 + \" kB\")\n",
    "println(\"Parquet size: \" + fs.listStatus(new Path(path + \"pi.parquet\")).map(x => x.getBlockSize()).sum / 1024 + \" kB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
