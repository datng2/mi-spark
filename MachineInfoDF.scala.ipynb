{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform MachineInfo from RDD to DataFrames\n",
    "\n",
    "### TL;DR:\n",
    "\n",
    "`Parqet` seems to reduce storage cost and `DataFrames` allows to make complex queries.\n",
    "\n",
    "### Motivation\n",
    "Expose information from MachineInfo as Spark’s DataFrames to provide SQL-based offline or on-demand analysis capabilities.\n",
    "\n",
    "### Overview\n",
    "* Extract and flatten MachineInfo to a couple of tables such as Processes, Events, Workloads, etc. Save the table in column-oriented `parquet` format to reduce the required disk space and hence improve the retention of these tables.\n",
    "* Explore other ways to reduce the disk usage.\n",
    "* Propose a couple of real-life usages of these DataFrames for Threat Hunting using Spark’s built-in aggregation functions and plugged-in user-defined functions.\n",
    "\n",
    "### Future work\n",
    "* Convert experimental code into `PsPipline` codebase.\n",
    "* Write tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import  dependencies\n",
    "\n",
    "* Assume that `PsPipeline` has been compiled into a jar file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///Users/datng2/pspipline.jar\n",
      "Finished download of pspipline.jar\n"
     ]
    }
   ],
   "source": [
    "// Ref: https://toree.apache.org/docs/current/user/faq/\n",
    "%AddJar file:///Users/datng2/pspipline.jar\n",
    "sc.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@6d92d0f0\n",
       "sqlContext = org.apache.spark.sql.SQLContext@55a2f19b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@55a2f19b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "// https://stackoverflow.com/questions/44094108/not-able-to-import-spark-implicits-in-scalatest\n",
    "val spark2 = spark\n",
    "val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n",
    "import spark2.implicits._\n",
    "import sqlContext.implicits._\n",
    "\n",
    "import java.net.URI\n",
    "import collection.JavaConverters._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import com.tetration.proto.MachineInfoProtos.MachineInfo\n",
    "import com.tetration.processanalytics.pipeline.io.MachineInfoBatchReaderWithProtoField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load `RDD[MachineInfo]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATA_DIR = /Users/datng2/data/machineinfo/galois\n",
       "NUM_BATCHES = 8\n",
       "batches = WrappedArray(201805091922, 201805092101, 201805091925, 201805091913, 201805091914, 201805091940, 201805091947, 201805091949)\n",
       "miProtoReader = com.tetration.processanalytics.pipeline.io.MachineInfoBatchReaderWithProtoField@1c77331f\n",
       "miRDDs = UnionRDD[38] at union at <console>:77\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[38] at union at <console>:77"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.URI\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "final val DATA_DIR = \"/Users/datng2/data/machineinfo/galois\"\n",
    "final val NUM_BATCHES = 8\n",
    "\n",
    "// Print all files in a directory\n",
    "val batches = FileSystem\n",
    "    .get(URI.create(\"file:///\"), sc.hadoopConfiguration)\n",
    "    .listStatus(new Path(DATA_DIR))\n",
    "    .map(_.getPath.toString.split(\"/\").last)  // extract filename from an absolute path\n",
    "    .take(NUM_BATCHES)  // select first N-items \n",
    "    .toSeq    //convert to seq\n",
    "\n",
    "val miProtoReader = new MachineInfoBatchReaderWithProtoField(\n",
    "    sc,\n",
    "    sc.hadoopConfiguration,\n",
    "    FileSystem.get(URI.create(\"file:///\"), sc.hadoopConfiguration),\n",
    "    DATA_DIR)\n",
    "\n",
    "val miRDDs = miProtoReader\n",
    "    .readBatches(batches.asJava).asScala\n",
    "    .reduce(_ union _)\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define  an experimental schema\n",
    "\n",
    "* The idea is to convert **RDD[MachineInfo] into RDD[Row]**, where each Row contains the information that our pipeline requires. In Spark 2.0, DataFrame is a type alias for *Dataset of Row*.\n",
    "* We could potentially consider leveraging `protobuf` definition to transform RDD to DataFrame. However, we currently use a simplified schema for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ProcessInfoRow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class ProcessInfoRow(\n",
    "    val sensorId: String,\n",
    "    val processKeyPart1: Long,\n",
    "    val processKeyPart2: Long,\n",
    "    val cpuUsage: Long,\n",
    "    val memoryUsage: Long,\n",
    "    val commandString: String\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIToProcessInfo = > List[ProcessInfoRow] = <function1>\n",
       "processInfoDF = [sensorId: string, processKeyPart1: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.catalyst.QualifiedTableName'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'QualifiedTableName.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sensorId: string, processKeyPart1: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform functions\n",
    "val MIToProcessInfo: MachineInfo => List[ProcessInfoRow] = mi => {\n",
    "  mi.getProcessInfoList.asScala\n",
    "    .map(pi => ProcessInfoRow(\n",
    "        mi.getSensorId,\n",
    "        pi.getKey.getPart1,\n",
    "        pi.getKey.getPart2,\n",
    "        pi.getCpuUsageUs,\n",
    "        pi.getMemoryUsageKB,\n",
    "        pi.getCommandString))\n",
    "    .toList\n",
    "}\n",
    "\n",
    "val processInfoDF = miRDDs\n",
    "    .flatMap(MIToProcessInfo)\n",
    "    .toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sample Queries:\n",
    "Ref: \n",
    "1. https://github.com/apache/incubator-toree/blob/master/etc/examples/notebooks/magic-tutorial.ipynb\n",
    "2. https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|      avg(cpuUsage)|\n",
      "+-------------------+\n",
      "|4.006694183496826E9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Create a ProcessInfo Table\n",
    "processInfoDF.createOrReplaceTempView(\"ProcessInfo\")\n",
    "\n",
    "// Compute average CPU usage (in microsecond)\n",
    "sqlContext\n",
    "    .sql(\"SELECT AVG(cpuUsage) FROM ProcessInfo\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    5199|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Count number of rows sensorId\n",
    "sqlContext.sql(\"SELECT COUNT(*) FROM ProcessInfo\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------+\n",
      "|            sensorId|count(DISTINCT commandString)|\n",
      "+--------------------+-----------------------------+\n",
      "|fda22d272815f1503...|                          317|\n",
      "|7f3fc376d08f771af...|                          434|\n",
      "|11f991d882bdab1f7...|                          307|\n",
      "|20bee197e8368b81b...|                          315|\n",
      "|a26526ab2b11a0a1c...|                          318|\n",
      "|967d3582a6a9bc0da...|                          399|\n",
      "+--------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Create a SQL query to count number of unique command strings in each sensorId\n",
    "sqlContext.sql(\"\"\"\n",
    "    SELECT sensorId, COUNT(DISTINCT commandString) \n",
    "        FROM ProcessInfo\n",
    "        GROUP BY sensorId\n",
    "    \"\"\")\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensorId: string (nullable = true)\n",
      " |-- processKeyPart1: long (nullable = false)\n",
      " |-- processKeyPart2: long (nullable = false)\n",
      " |-- cpuUsage: long (nullable = false)\n",
      " |-- memoryUsage: long (nullable = false)\n",
      " |-- commandString: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processInfoDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Saving to Parquet vs RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.io.compress.GzipCodec\n",
    "sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "\n",
    "// Saving data to disk\n",
    "processInfoDF.write.parquet(\"pi.parquet\")\n",
    "processInfoDF.rdd.saveAsTextFile(\"pi.rdd\", classOf[GzipCodec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD output size    : 6476005 kB\n",
      "Parquet output size: 1275068 kB\n",
      "Saving Percecentage: 81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /Users/datng2/internship/mi-spark/\n",
       "fs = org.apache.hadoop.fs.LocalFileSystem@7b6f5f17\n",
       "rddSize = 6476005376\n",
       "parquetSize = 1275068416\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1275068416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute size of RDD vs Parquet\n",
    "val path = \"/Users/datng2/internship/mi-spark/\"\n",
    "\n",
    "val fs = FileSystem.get(URI.create(\"file:///\"), sc.hadoopConfiguration)\n",
    "\n",
    "val rddSize = fs.listStatus(new Path(path + \"pi.rdd\"))\n",
    "    .map(x => x.getBlockSize())\n",
    "    .sum\n",
    "\n",
    "val parquetSize = fs.listStatus(new Path(path + \"pi.parquet\"))\n",
    "    .map(x => x.getBlockSize())\n",
    "    .sum\n",
    "\n",
    "// Display result\n",
    "println(\"RDD output size    : \" + rddSize / 1000 + \" kB\")\n",
    "println(\"Parquet output size: \" + parquetSize / 1000 + \" kB\")\n",
    "println(\"Saving Percecentage: \" + (100 - (parquetSize * 100/ rddSize)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
